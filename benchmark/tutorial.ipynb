{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spatial Clustering of Molecular Localizations Using MIRO**\n",
    "\n",
    "This tutorial demonstrates how to train and apply **MIRO** to cluster spatial point clouds obtained from single-molecule localization microscopy (SMLM). Specifically, we show how to use MIRO on benchmark data introduced in [Nieves et al., *Nature Methods* (2023)](https://www.nature.com/articles/s41592-022-01750-6), which provides a standardized framework for evaluating clustering methods in SMLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Download the data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, download the benchmark dataset using the `sdownload` function. \n",
    "\n",
    "This function requires three parameters: `id`, which specifies the dataset to download (in this case, `id=6`); `local_folder`, the target directory where the data will be stored; and `blinking`, a boolean indicating whether to include fluorophore blinking effects. \n",
    "\n",
    "The function returns the path to the downloaded dataset, which will be used for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download import sdownload\n",
    "\n",
    "BLINKING = False\n",
    "path = sdownload(id=6, local_folder=\"data\", blinking=BLINKING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the training data and extract a small set of representative clusters for training. \n",
    "\n",
    "One of **MIRO**’s key advantages is its ability to learn effectively from limited data, typically requiring only a few dozen well-chosen clusters to generalize successfully.\n",
    "\n",
    "In this tutorial, we use clusters extracted from just 3 out of the 50 available images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the metadata associated with the dataset\n",
    "with open(\"metadata.json\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "data_dir = Path(path)\n",
    "\n",
    "NUM_RE = re.compile(r\"\\d+\")\n",
    "csv_files = sorted(\n",
    "    data_dir.glob(\"*.csv\"), key=lambda p: int(NUM_RE.search(p.name).group())\n",
    ")\n",
    "\n",
    "metadata = metadata[data_dir.name]\n",
    "training_indices = metadata[\"training_indices\"]\n",
    "training_paths = [csv_files[i - 1] for i in training_indices]\n",
    "\n",
    "training_clusters, training_backgrounds = [], []\n",
    "for csv_path in training_paths:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Normalize coordinates to [0, 1]\n",
    "    coords = df[[\"x\", \"y\"]]\n",
    "    mins, maxs = coords.min(), coords.max()\n",
    "    df[[\"x\", \"y\"]] = (coords - mins) / (maxs - mins)\n",
    "\n",
    "    # Group by 'index'\n",
    "    for idx, group in df.groupby(\"index\"):\n",
    "        # 0 (the background)\n",
    "        if idx == 0:\n",
    "            training_backgrounds.append(group[[\"x\", \"y\"]].values)\n",
    "        else:\n",
    "            centered = group[[\"x\", \"y\"]] - group[[\"x\", \"y\"]].mean()\n",
    "            training_clusters.append(centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Build the training dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MIRO** employs an augmentation pipeline that transforms the small set of training cluster into a large number of diverse point clouds for training. \n",
    "\n",
    "Each point cloud is generated by applying a series of transformations to randomly selected clusters, including geometric transformations (rotations, reflections), stochastic perturbations (localization dropout or addition), and spatial jitter (small random displacements).  These transformed clusters are then randomly placed within a synthetic FOV to generate the final training samples.\n",
    "\n",
    "\n",
    "The following code initializes the **MIRO** data builder, which generates the augmented training dataset based on the extracted clusters and a set of metadata parameters. These metadata settings define key aspects of the synthetic dataset, such as the number of generated samples, the range of background noise points, the number of clusters per FOV, and the spatial connectivity radius used to define cluster compactness. \n",
    "\n",
    "You can inspect these settings by printing `metadata[\"builder_kwargs\"]`. Once initialized, calling the builder returns a fully augmented dataset ready for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the library path is in sys.path\n",
    "lib_path = Path.cwd().parent.resolve()\n",
    "\n",
    "if str(lib_path) not in sys.path:\n",
    "    sys.path.insert(0, str(lib_path))\n",
    "\n",
    "import lib\n",
    "\n",
    "# Initialize the MIRO builder with the training clusters and metadata\n",
    "builder_args = (training_clusters,)\n",
    "if BLINKING:\n",
    "    builder_args += (training_backgrounds,)\n",
    "    \n",
    "builder = getattr(lib, metadata[\"builder\"])(\n",
    "    *builder_args, **metadata[\"builder_kwargs\"]\n",
    ")\n",
    "augmented_dataset = builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a closer look at one of the augmented training images.\n",
    "\n",
    "In **MIRO**, each training sample is represented as a **graph**, where nodes correspond to individual molecular localizations and edges are defined via Delaunay triangulation to capture local spatial relationships. You can use the following code to visualize one of these examples.\n",
    "\n",
    "The colormap in the labeled scatter plot represents the magnitude of the **displacement vectors** from each node to its assigned cluster center. These vectors serve as the ground truth during training, enabling **MIRO** to learn transformations that contract structures toward a common center, effectively modeling the underlying cluster geometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_data(sample_idx):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    data = augmented_dataset[sample_idx]\n",
    "\n",
    "    # Plot the graph representation\n",
    "    for edge in data[\"edge_index\"].T:\n",
    "        x = [data[\"position\"][edge[0]][0], data[\"position\"][edge[1]][0]]\n",
    "        y = [data[\"position\"][edge[0]][1], data[\"position\"][edge[1]][1]]\n",
    "        ax[0].plot(x, y, color=\"black\", linewidth=0.5, alpha=0.3)\n",
    "    ax[0].scatter(\n",
    "        data[\"position\"][:, 0],\n",
    "        data[\"position\"][:, 1],\n",
    "        s=10,\n",
    "        zorder=2,\n",
    "        c=\"orange\",\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_title(\"Input Graph\", fontsize=10)\n",
    "\n",
    "    # Plot the ground truth displacement vectors\n",
    "    ax[1].scatter(\n",
    "        data[\"position\"][:, 0],\n",
    "        data[\"position\"][:, 1],\n",
    "        c=np.linalg.norm(data[\"y\"], axis=1),\n",
    "        s=10,\n",
    "    )\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    ax[1].set_title(\"Ground Truth\", fontsize=10)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.02)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "random_sample_idx = np.random.randint(0, len(augmented_dataset))\n",
    "plot_training_data(random_sample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Create the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate **MIRO** using the configuration that matches the training data. \n",
    "\n",
    "The model is initialized with a set of key parameters: the number of output features (in this case, 2 for the predicted *x* and *y* displacements), the connectivity radius used to construct the input graphs (which should match the one used during augmentation), and the number of recurrent message-passing iterations to apply on the input graphs (set to 20 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "clusterer = dl.MIRO(\n",
    "    num_outputs=2,  # Number of output features (e.g., x, y displacements)\n",
    "    connectivity_radius=builder.connectivity_radius,  # Radius for graph connectivity (matches dataset)\n",
    "    num_iterations=metadata[\"recurrent_iterations\"],  # Number of iterations for graph processing\n",
    ")\n",
    "clusterer = clusterer.create()\n",
    "\n",
    "print(clusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Train the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the data loaders and configure the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=augmented_dataset,  # The dataset to be loaded\n",
    "    batch_size=4,  # Number of samples per batch\n",
    "    shuffle=True,  # Shuffle the dataset at every epoch\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = dl.Trainer(max_epochs=30)  # Maximum number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train **MIRO**. \n",
    "\n",
    "Alternatively, you can load a pre-trained model by setting `train_model = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_model = False\n",
    "\n",
    "if train_model:\n",
    "    trainer.fit(\n",
    "        clusterer,  # The MIRO model to be trained\n",
    "        train_loader,  # The DataLoader providing the training data\n",
    "    )\n",
    "else:\n",
    "    clusterer.load_state_dict(torch.load(metadata[\"checkpoint\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Test the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, it’s time to evaluate **MIRO** on the validation dataset.\n",
    "\n",
    "The following code reads the validation set and applies the trained model to perform clustering. The `clusterer` object includes a built-in `.clustering()` method, which automates the full inference pipeline. This method applies MIRO to each input graph, transforms the node features into the learned **squeezed representation** (where localizations belonging to the same cluster are pulled toward a common center) and runs DBSCAN on this transformed space to produce the final cluster assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clusterer.eval()\n",
    "clusterer.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "validation_paths = [path for path in csv_files if path not in training_paths]\n",
    "\n",
    "vresults = []\n",
    "for i, vpath in enumerate(validation_paths):\n",
    "    vdata = pd.read_csv(vpath).copy()\n",
    "    val_graph = lib.compute_test_graph(vdata, builder)\n",
    "\n",
    "    clusters = clusterer.clustering(\n",
    "        val_graph,  # Input graph representing the validation point cloud\n",
    "        eps=metadata[\"MIRO_DBSCAN_params\"][\n",
    "            \"eps\"\n",
    "        ],  # DBSCAN epsilon parameter for neighborhood radius\n",
    "        min_samples=metadata[\"MIRO_DBSCAN_params\"][\n",
    "            \"min_samples\"\n",
    "        ],  # DBSCAN minimum samples parameter for core points\n",
    "        scaling=vdata[[\"x\", \"y\"]]\n",
    "        .max()\n",
    "        .values,  # Used to convert displacements back to the original coordinate scale\n",
    "    )\n",
    "\n",
    "    dbclusters = DBSCAN(\n",
    "        eps=metadata[\"DBSCAN_params\"][\"eps\"],\n",
    "        min_samples=metadata[\"DBSCAN_params\"][\"min_samples\"],\n",
    "    ).fit_predict(vdata[[\"x\", \"y\"]])\n",
    "\n",
    "    vdata[\"clustering-MIRO\"] = clusters + 1\n",
    "    vdata[\"clustering-DBSCAN\"] = dbclusters + 1\n",
    "    vdata[\"set\"] = i\n",
    "    vresults.append(vdata)\n",
    "\n",
    "vresults = pd.concat(vresults, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below computes average evaluation metrics across 47 validation images. These include `ARI_values`, `AMI_values`, and class-imbalance-aware variants like `ARI_c_values` and `ARI_dagger_values`. We also report `IoU_values` for cluster overlap and cluster-level metrics such as `JIc_values`, `RMSRE_N_values`, and `RMSE_centr_values`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lib.calculate_metrics_for_experiments(vresults)\n",
    "aresults = results.groupby('class_names').mean().reset_index().round(2)\n",
    "\n",
    "aresults = aresults.drop(columns=['experiment'])\n",
    "aresults = aresults.set_index('class_names').transpose()\n",
    "aresults.columns.name = None\n",
    "\n",
    "print(\"Average Results:\")\n",
    "print(aresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize a randomly selected validation image along with its predicted cluster assignments from DBSCAN, MIRO-enhanced DBSCAN, and the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_results(results):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Titles for each subplot\n",
    "    titles = [\"DBSCAN\", \"MIRO-enhanced\", \"Ground Truth\"]\n",
    "\n",
    "    # Loop through each clustering method and visualize results\n",
    "    for i, c in enumerate([\"clustering-DBSCAN\", \"clustering-MIRO\", \"index\"]):\n",
    "        clustering = results[c].copy()\n",
    "        ax[i].set_xticks([])  # Remove x-axis ticks for a cleaner plot\n",
    "        ax[i].set_yticks([])  # Remove y-axis ticks for a cleaner plot\n",
    "        ax[i].set_title(titles[i], fontsize=10)  # Set title for the subplot\n",
    "\n",
    "        # Iterate through unique cluster labels\n",
    "        for u in np.unique(clustering):\n",
    "            if u == 0:  # Background points (label 0)\n",
    "                ax[i].scatter(\n",
    "                    results[\"x\"][results[c] == u],  # x-coordinates of background points\n",
    "                    results[\"y\"][results[c] == u],  # y-coordinates of background points\n",
    "                    s=4,  # Point size\n",
    "                    c=\"gray\",  # Background color\n",
    "                    alpha=0.5,  # Transparency for background points\n",
    "                )\n",
    "            else:  # Clustered points\n",
    "                ax[i].scatter(\n",
    "                    results[\"x\"][results[c] == u],  # x-coordinates of clustered points\n",
    "                    results[\"y\"][results[c] == u],  # y-coordinates of clustered points\n",
    "                    s=4,  # Point size\n",
    "                    c=np.random.rand(3),  # Random RGB color for each cluster\n",
    "                )\n",
    "    plt.subplots_adjust(wspace=0.02)  # Adjust spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "validation_image = vresults[\n",
    "    vresults[\"set\"] == np.random.choice(vresults[\"set\"].unique())\n",
    "].copy()\n",
    "plot_clustering_results(validation_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
