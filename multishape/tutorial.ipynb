{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multiscale Clustering and Classification of Molecular Localizations Using MIRO**\n",
    "\n",
    "This tutorial demonstrates how to train and apply **MIRO** for the simultaneous clustering and classification of spatial point clouds from single-molecule localization microscopy (SMLM).\n",
    "\n",
    "We show how to train **MIRO** on synthetic data that captures multiscale structural variability, and then evaluate its ability to recover both cluster assignments and underlying geometric classes. The tutorial also includes guidance on building custom data loaders tailored to the structure and format of each dataset, an essential step for applying MIRO to diverse and realistic scenarios in SMLM analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the library path is in sys.path\n",
    "lib_path = Path.cwd().parent.resolve()\n",
    "\n",
    "if str(lib_path) not in sys.path:\n",
    "    sys.path.insert(0, str(lib_path))\n",
    "\n",
    "import lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Load the Data**\n",
    "\n",
    "To begin, load the training data.\n",
    "\n",
    "It consists of pre-generated synthetic images containing mixtures of circular and elliptical clusters. Each cluster type represents a distinct molecular assembly, characterized by its own spatial geometry and scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_data = pd.read_csv(\"data/training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Build the training dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the training dataset by converting each synthetic image into a graph-based representation compatible with **MIRO**. The code snippet below demonstrates how to build a custom data loader tailored to this multiscale setting.\n",
    "\n",
    "The `MultiscaleGraphDataset` class reads each training image and constructs a graph where nodes represent individual molecular localizations, and edges are defined using Delaunay triangulation. These edges are then filtered using a specified `connectivity_radius` to retain only meaningful local connections.\n",
    "\n",
    "For each image, the loader returns a `torch_geometric.data.Data` object containing the graph structure, positional features, ground truth displacement vectors, and node-wise shape labels. These data objects serve as the input to the **MIRO** model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.tri as tri\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class MultiscaleGraphDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        training_images,\n",
    "        connectivity_radius=1,\n",
    "    ):\n",
    "        self.training_images = training_images\n",
    "        self.connectivity_radius = connectivity_radius\n",
    "\n",
    "        self.laplacian_embedding = AddLaplacianEigenvectorPE(\n",
    "            5, attr_name=\"x\", is_undirected=True\n",
    "        )\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.generate_dataset()\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        \"\"\"Generate a dataset of graph-structured data.\"\"\"\n",
    "        dataset = []\n",
    "        for _, df_set in tqdm.tqdm_notebook(\n",
    "            self.training_images.groupby(\"set\", sort=False),\n",
    "            desc=\"Generating dataset\",\n",
    "        ):\n",
    "            positions = df_set[[\"x\", \"y\"]].to_numpy()\n",
    "            deltas = df_set[[\"dx\", \"dy\"]].to_numpy()\n",
    "            classes = df_set[\"class\"].to_numpy()\n",
    "\n",
    "            data = self.create_data_object(positions, deltas, classes)\n",
    "            dataset.append(data)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_data_object(self, positions, deltas, classes):\n",
    "        \"\"\"Create a `Data` object from positions and deltas.\"\"\"\n",
    "        data = Data()\n",
    "\n",
    "        # Prepare ground truth displacements\n",
    "        data.y, data.y_class = (\n",
    "            torch.tensor(deltas, dtype=torch.float) / self.connectivity_radius,\n",
    "            torch.tensor(classes, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        # Add graph structure\n",
    "        data.edge_index, data.edge_attr = self.compute_connectivity(positions)\n",
    "        data.num_nodes = positions.shape[0]\n",
    "\n",
    "        # Add positional and embedding features\n",
    "        data = self.laplacian_embedding(data)\n",
    "        data.x = torch.abs(data.x)\n",
    "        data.position = torch.tensor(positions, dtype=torch.float)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def compute_connectivity(self, positions):\n",
    "        \"\"\"Compute the connectivity graph and edge attributes.\"\"\"\n",
    "        delaunay = tri.Triangulation(positions[:, 0], positions[:, 1])\n",
    "        edges = self.extract_edges(delaunay.triangles)\n",
    "\n",
    "        distances, displacements = self.compute_edge_metrics(edges, positions)\n",
    "        valid_mask = distances < self.connectivity_radius\n",
    "\n",
    "        edges = edges[valid_mask]\n",
    "        distances = distances[valid_mask, None]\n",
    "        displacements = displacements[valid_mask]\n",
    "\n",
    "        edges, displacements, distances = self.make_graph_undirected(\n",
    "            edges, displacements, distances\n",
    "        )\n",
    "\n",
    "        edge_index, edge_attr = self.format_edges_and_attributes(\n",
    "            edges, displacements, distances\n",
    "        )\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_edges(triangles):\n",
    "        \"\"\"Extract edges from triangles.\"\"\"\n",
    "        edges = [\n",
    "            np.array(list(itertools.combinations(triangle, r=2)))[[1, 0, 2]]\n",
    "            for triangle in triangles\n",
    "        ]\n",
    "        return np.concatenate(edges, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_edge_metrics(edges, positions):\n",
    "        \"\"\"Compute displacements and distances for edges.\"\"\"\n",
    "        displacements = positions[edges[:, 0]] - positions[edges[:, 1]]\n",
    "        distances = np.linalg.norm(displacements, axis=1)\n",
    "        return distances, displacements\n",
    "\n",
    "    @staticmethod\n",
    "    def make_graph_undirected(edges, displacements, distances):\n",
    "        \"\"\"Ensure the graph is undirected by mirroring edges.\"\"\"\n",
    "        edges = np.concatenate([edges, edges[:, [1, 0]]], axis=0)\n",
    "        displacements = np.concatenate([displacements, -displacements], axis=0)\n",
    "        distances = np.concatenate([distances, distances], axis=0)\n",
    "\n",
    "        # Remove duplicate edges\n",
    "        unique_edges, indices = np.unique(edges, axis=0, return_index=True)\n",
    "        return unique_edges, displacements[indices], distances[indices]\n",
    "\n",
    "    def format_edges_and_attributes(self, edges, displacements, distances):\n",
    "        \"\"\"Format edges and their attributes for PyTorch Geometric.\"\"\"\n",
    "        edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "        edge_attr = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    torch.tensor(displacements, dtype=torch.float),\n",
    "                    torch.tensor(distances, dtype=torch.float),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            / self.connectivity_radius\n",
    "        )\n",
    "        return edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate the dataset builder and generate the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = MultiscaleGraphDataset(\n",
    "    training_data,\n",
    "    connectivity_radius=0.2,\n",
    ")\n",
    "training_dataset = builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a closer look at one of the training samples.\n",
    "\n",
    "From left to right, we visualize the graph representation, the displacement vectors pointing from each node to its assigned cluster center, and the ground truth shape labels, where each node is colored according to the class of the cluster it belongs to (e.g., circular or elliptical).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def plot_training_data(data, figsize=(15, 5), titles=None):\n",
    "    pos = data[\"position\"]        \n",
    "    edges = data[\"edge_index\"].T  \n",
    "    values = [                    \n",
    "        None,\n",
    "        np.linalg.norm(data[\"y\"], axis=1),\n",
    "        data[\"y_class\"]\n",
    "    ]\n",
    "\n",
    "    titles = titles or [\n",
    "        \"Input Graph\",\n",
    "        \"Ground Truth (displacements)\",\n",
    "        \"Ground Truth (classes)\",\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize, constrained_layout=True)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(titles[i], fontsize=16)\n",
    "\n",
    "        if i == 0:\n",
    "            segments = pos[edges]\n",
    "            lc = LineCollection(segments, colors=\"black\",\n",
    "                                linewidths=0.5, alpha=0.3)\n",
    "            ax.add_collection(lc)\n",
    "            ax.scatter(pos[:, 0], pos[:, 1],\n",
    "                       s=10, c=\"orange\", edgecolors=\"black\", zorder=2)\n",
    "        else:\n",
    "            im = ax.scatter(pos[:, 0], pos[:, 1],\n",
    "                            c=values[i], s=10)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "idx = random.randrange(len(training_dataset))\n",
    "fig, axes = plot_training_data(training_dataset[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Create the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate **MIRO** using a configuration that matches the training data.\n",
    "\n",
    "The model is initialized with a set of key parameters: the number of classes (in this case, 3, corresponding to background, circular clusters, and elliptical clusters), the connectivity radius used to construct the input graphs (which should match the value used during augmentation), and the number of recurrent message-passing iterations applied to each graph (set to 20 by default).\n",
    "\n",
    "In this configuration, the total number of output features is set to `num_classes + 2`, accounting for both the predicted *x* and *y* displacements, as well as the per-node class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "clusterer = dl.cMIRO(\n",
    "    num_classes=3,  # Total number of features is 2 (i.e., x, y displacements) + num_classes\n",
    "    connectivity_radius=builder.connectivity_radius,  # Radius for graph connectivity (matches dataset)\n",
    "    num_iterations=20,  # Number of iterations for graph processing\n",
    ")\n",
    "clusterer = clusterer.create()\n",
    "\n",
    "print(clusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Train the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the data loaders and configure the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=training_dataset,  # The dataset to be loaded\n",
    "    batch_size=4,  # Number of samples per batch\n",
    "    shuffle=True,  # Shuffle the dataset at every epoch\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = dl.Trainer(max_epochs=30)  # Maximum number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train **MIRO**. \n",
    "\n",
    "Alternatively, you can load a pre-trained model by setting `train_model = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_model = False\n",
    "\n",
    "if train_model:\n",
    "    trainer.fit(\n",
    "        clusterer,  # The MIRO model to be trained\n",
    "        train_loader,  # The DataLoader providing the training data\n",
    "    )\n",
    "else:\n",
    "    clusterer.load_state_dict(torch.load(\"models/checkpoint.pt\"))  # Load pre-trained model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Test the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, it’s time to evaluate **MIRO** on the validation dataset.\n",
    "\n",
    "The following code reads the validation set and applies the trained model to perform simultaneous clustering and classification. The `clusterer` object provides a built-in `.clustering()` method that automates the full inference pipeline. This method applies **MIRO** to each input graph, transforms the node features into a learned **squeezed representation**, where localizations belonging to the same cluster are pulled toward a common center; runs DBSCAN on this transformed space to generate final cluster assignments; and refines the predicted node-wise class labels based on the clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "clusterer.eval()\n",
    "clusterer.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "validation_data = pd.read_csv(\"data/validation_data.csv\")\n",
    "\n",
    "vresults = []\n",
    "for s in validation_data[\"set\"].unique():\n",
    "    vdata = validation_data[validation_data[\"set\"] == s].copy()\n",
    "    val_graph = lib.compute_test_graph(vdata, builder, norm=False)\n",
    "\n",
    "    clusters, classes = clusterer.clustering(\n",
    "        val_graph,  # Input graph representing the validation point cloud\n",
    "        eps=0.013,  # DBSCAN epsilon parameter for neighborhood radius\n",
    "        min_samples=5,  # DBSCAN minimum samples parameter for core points\n",
    "    )\n",
    "\n",
    "    vdata[\"clustering-MIRO\"] = clusters\n",
    "    vdata[\"class-MIRO\"] = classes\n",
    "    vresults.append(vdata)\n",
    "\n",
    "vresults = pd.concat(vresults, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize a randomly selected validation image along with its predicted cluster assignments and corresponding classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def plot_evaluation(df):\n",
    "    pos = df[[\"x\", \"y\"]].to_numpy()\n",
    "    cluster_gt = df[\"index\"].to_numpy()\n",
    "    cluster_pr = df[\"clustering-MIRO\"].to_numpy()\n",
    "    class_gt = df[\"class\"].to_numpy()\n",
    "    class_pr = df[\"class-MIRO\"].to_numpy()\n",
    "\n",
    "    def _draw_points(ax):\n",
    "        ax.scatter(*pos.T, s=10, c=\"black\", alpha=0.5)\n",
    "\n",
    "    def _draw_clusters(ax, values):\n",
    "        # noise/background\n",
    "        mask_noise = values == -1\n",
    "        if mask_noise.any():\n",
    "            ax.scatter(\n",
    "                *pos[mask_noise].T,\n",
    "                s=10,\n",
    "                c=\"gray\",\n",
    "                alpha=0.5,\n",
    "                label=\"_noise\",\n",
    "            )\n",
    "        # clusters\n",
    "        mask_valid = ~mask_noise\n",
    "        cmap = plt.get_cmap(\"tab20\")\n",
    "        ax.scatter(\n",
    "            *pos[mask_valid].T,\n",
    "            c=values[mask_valid],\n",
    "            s=10,\n",
    "            cmap=cmap,\n",
    "        )\n",
    "\n",
    "    def _draw_classes(ax, values):\n",
    "        cmap = plt.get_cmap(\"viridis\")\n",
    "        ax.scatter(*pos.T, c=values, s=10, cmap=cmap)\n",
    "\n",
    "    panel_funcs = [\n",
    "        _draw_points,\n",
    "        partial(_draw_clusters, values=cluster_gt),\n",
    "        partial(_draw_clusters, values=cluster_pr),\n",
    "        partial(_draw_classes, values=class_gt),\n",
    "        partial(_draw_classes, values=class_pr),\n",
    "    ]\n",
    "\n",
    "    titles = [\n",
    "        \"Point cloud\",\n",
    "        \"Gound-truth clustering\",\n",
    "        \"MIRO clustering\",\n",
    "        \"Gound-truth classes\",\n",
    "        \"MIRO classes\",\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(8, 6), constrained_layout=True)\n",
    "    # turn off the empty subplot at (1,0)\n",
    "    axes[1, 0].axis(\"off\")\n",
    "\n",
    "    plot_axes = [\n",
    "        axes[0, 0],\n",
    "        axes[0, 1],\n",
    "        axes[0, 2],\n",
    "        axes[1, 1],\n",
    "        axes[1, 2],\n",
    "    ]\n",
    "\n",
    "    for ax, title, draw in zip(plot_axes, titles, panel_funcs):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        draw(ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "validation_image = vresults[\n",
    "    vresults[\"set\"] == np.random.choice(vresults[\"set\"].unique())\n",
    "].copy()\n",
    "plot_evaluation(validation_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
